{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aylNQKxtOtoY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b058da0-8c9d-4d9c-a3f2-0d564df30bfd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "import string\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import random\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "g5JkFCKbXhEY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"/content/drive/MyDrive/textset\"\n",
        "def read_all_textfile(path):\n",
        "    txt = []\n",
        "    if not os.path.exists(path):\n",
        "        print(\"Error: Path does not exist.\")\n",
        "        return txt\n",
        "\n",
        "    for root, _, files in os.walk(path):\n",
        "        for file in files:\n",
        "            file_path = os.path.join(root, file)\n",
        "           # print(\"Reading file:\", file_path)\n",
        "            with open(file_path) as f:\n",
        "                for line in f:\n",
        "                    line = line.strip()\n",
        "                    if line == '----------':\n",
        "                        break\n",
        "                    if line != '':\n",
        "                        txt.append(line)\n",
        "    return txt\n",
        "\n",
        "\n",
        "arr = read_all_textfile(path)\n",
        "print(\"number of lines =\", len(arr))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w-dAymtjccyd",
        "outputId": "cdc36dbe-ba9e-4be7-a28e-59b45532cd87"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of lines = 215021\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download(\"popular\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k220TKaoxeax",
        "outputId": "80d615de-f6a9-4eae-a371-e909a7461a11"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'popular'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection popular\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def clean_txt(txt):\n",
        "    cleaned_txt = []\n",
        "    for line in txt:\n",
        "        line = line.lower()\n",
        "        line = re.sub(r\"[,.\\\"\\'!@#$%^&*(){}?/;`~:<>+=-\\\\]\", \"\", line)\n",
        "        tokens = word_tokenize(line)  # This line was causing the error\n",
        "        words = [word for word in tokens if word.isalpha()]\n",
        "        cleaned_txt += words\n",
        "    return cleaned_txt\n",
        "\n",
        "# Assuming 'arr' is defined elsewhere in your code\n",
        "cleaned_arr = clean_txt(arr)\n",
        "print(\"number of words =\", len(cleaned_arr))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T8-XDIaCvUzB",
        "outputId": "7c3aed18-5819-4416-ee75-f213d9414822"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of words = 2332247\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def make_markov_model(cleaned_arr, n_gram=2):\n",
        "    markov_model = {}\n",
        "    for i in range(len(cleaned_arr)-n_gram-1):\n",
        "        curr_state, next_state = \"\", \"\"\n",
        "        for j in range(n_gram):\n",
        "            curr_state += cleaned_arr[i+j] + \" \"\n",
        "            next_state += cleaned_arr[i+j+n_gram] + \" \"\n",
        "        curr_state = curr_state[:-1]\n",
        "        next_state = next_state[:-1]\n",
        "        if curr_state not in markov_model:\n",
        "            markov_model[curr_state] = {}\n",
        "            markov_model[curr_state][next_state] = 1\n",
        "        else:\n",
        "            if next_state in markov_model[curr_state]:\n",
        "                markov_model[curr_state][next_state] += 1\n",
        "            else:\n",
        "                markov_model[curr_state][next_state] = 1\n",
        "\n",
        "\n",
        "\n",
        "    # calculating transition probabilities\n",
        "    for curr_state, transition in markov_model.items():\n",
        "        total = sum(transition.values())\n",
        "        for state, count in transition.items():\n",
        "            markov_model[curr_state][state] = count/total\n",
        "\n",
        "    return markov_model"
      ],
      "metadata": {
        "id": "0ZkQkCkmKR_-"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "markov_model = make_markov_model(cleaned_arr)\n",
        "print(\"number of states = \", len(markov_model.keys()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zzx8rFAsMrd6",
        "outputId": "99ca74e4-1c1f-4ab8-89eb-1bb3d34034b1"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of states =  208718\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"All possible transitions from 'the game' state: \\n\")\n",
        "print(markov_model['the game'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GTVgbLLRM899",
        "outputId": "34331060-fbb5-4160-d447-684177363672"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All possible transitions from 'the game' state: \n",
            "\n",
            "{'is afoot': 0.036036036036036036, 'was up': 0.09009009009009009, 'for the': 0.036036036036036036, 'your letter': 0.02702702702702703, 'was whist': 0.036036036036036036, 'was in': 0.02702702702702703, 'is hardly': 0.02702702702702703, 'would have': 0.036036036036036036, 'is up': 0.06306306306306306, 'is and': 0.036036036036036036, 'in their': 0.036036036036036036, 'in that': 0.036036036036036036, 'the lack': 0.036036036036036036, 'for all': 0.06306306306306306, 'may wander': 0.02702702702702703, 'now a': 0.02702702702702703, 'my own': 0.02702702702702703, 'at any': 0.02702702702702703, 'mr holmes': 0.02702702702702703, 'ay whats': 0.02702702702702703, 'my friend': 0.02702702702702703, 'fairly by': 0.02702702702702703, 'is not': 0.02702702702702703, 'was not': 0.02702702702702703, 'was afoot': 0.036036036036036036, 'worth it': 0.02702702702702703, 'you are': 0.02702702702702703, 'i am': 0.02702702702702703, 'now count': 0.02702702702702703}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(markov_model, limit=100, start='my god'):\n",
        "    n = 0\n",
        "    curr_state = start\n",
        "    next_state = None\n",
        "    text = \"\"\n",
        "    text+=curr_state+\" \"\n",
        "    while n<limit:\n",
        "        next_state = random.choices(list(markov_model[curr_state].keys()),\n",
        "                                    list(markov_model[curr_state].values()))\n",
        "\n",
        "        curr_state = next_state[0]\n",
        "        text+=curr_state+\" \"\n",
        "        n+=1\n",
        "    return text"
      ],
      "metadata": {
        "id": "ED-wohg0NMH0"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(20):\n",
        "    print(str(i)+\". \", generate_text(markov_model, start=\"dear boy\", limit=8))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4JKt2EKhNZXl",
        "outputId": "83840902-0433-4613-cde6-c6f09dad6642"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.  dear boy it was to take them from the master about master godfrey he had not been at \n",
            "1.  dear boy just under the head of the street and immediately whistling for a few miles and then \n",
            "2.  dear boy children come said i ought to know where he was confined in an upper room under \n",
            "3.  dear boy just under one year of age broad shouldered with crisp curling black hair and an appropriate \n",
            "4.  dear boy douglas but it so you are in a walk with the long deptford reach and up \n",
            "5.  dear boy to apologise to him now i was in time for explanations she knows everything now and \n",
            "6.  dear boy to apologise to him must have heard of this steward married to the third night of \n",
            "7.  dear boy children come said i can follow you then roused his anger by calling him names at \n",
            "8.  dear boy children come said i very likely start does it no watson i fear that mr sherlock \n",
            "9.  dear boy to apologise to him when there were these german people and what do you say to \n",
            "10.  dear boy to apologise to him when you get there i never thought to spare upon such details \n",
            "11.  dear boy douglas but it so kindly read it aloud the note it is just possible that the \n",
            "12.  dear boy douglas but it so many have aspired to without success what do you know it as \n",
            "13.  dear boy just under the head of one mind with fear when mcmurdo awoke next morning thats quick \n",
            "14.  dear boy just under one year of age on one condition only it is unlikely that it is \n",
            "15.  dear boy it was he who had arranged for the college and got his account into the most \n",
            "16.  dear boy is the famous government expert sir james walter claims our first attention the house we were \n",
            "17.  dear boy to apologise to him tried again and again a reaction would seize him and that woman \n",
            "18.  dear boy children come said i it is one of the upper part of my argument the natural \n",
            "19.  dear boy to apologise to him in the direction of the profession which has been offered in this \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate_text(markov_model, start=\"the case\", limit=100))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v8pGJZHtNioD",
        "outputId": "d29093a0-052e-4474-e6c5-38bdc74af101"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the case was coming save the loss of the old uncle with his bogie hound the latter question he put his fingertips together as they fell upon him i understood the whole situation i dont say that there had been approached he laid it across holmess knee i rose and showed a sympathy which was not due to the fact that i wished then if it costs me my neck holmes smoked for some few minutes i could have done enough for you not the bearing of this mans service that he were here i was too hard then go to london where he crouched by the window above you sterndale sprang to his throat to stifle his inclination to call out at the door to it these four and five year olds second third new course one mile and five furlongs mr heath newtons the negro red cap cinnamon jacket colonel wardlaws pugilist pink cap blue and silver but a black coat thrown over his shoulder strode off down the street holmes burst out laughing as we left them all round the hole and that third person have no such aid and yet whom by the deplorable laws of england i \n"
          ]
        }
      ]
    }
  ]
}